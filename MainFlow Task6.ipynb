{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132f3a3e-257d-4234-ac8b-9be204160809",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Tasks to Perform\n",
    "a.Load the Dataset\n",
    "○ Check for missing or inconsistent data.\n",
    "○ Clean the dataset by handling null values and duplicate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64aae3dd-b8fe-4ad0-923d-18b2bceaf65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values:\n",
      "age         0\n",
      "sex         0\n",
      "cp          0\n",
      "trestbps    0\n",
      "chol        0\n",
      "fbs         0\n",
      "restecg     0\n",
      "thalach     0\n",
      "exang       0\n",
      "oldpeak     0\n",
      "slope       0\n",
      "ca          0\n",
      "thal        0\n",
      "target      0\n",
      "dtype: int64\n",
      "\n",
      "Number of duplicate rows: 1\n",
      "\n",
      "Dataset Info After Cleaning:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 302 entries, 0 to 302\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       302 non-null    int64  \n",
      " 1   sex       302 non-null    int64  \n",
      " 2   cp        302 non-null    int64  \n",
      " 3   trestbps  302 non-null    int64  \n",
      " 4   chol      302 non-null    int64  \n",
      " 5   fbs       302 non-null    int64  \n",
      " 6   restecg   302 non-null    int64  \n",
      " 7   thalach   302 non-null    int64  \n",
      " 8   exang     302 non-null    int64  \n",
      " 9   oldpeak   302 non-null    float64\n",
      " 10  slope     302 non-null    int64  \n",
      " 11  ca        302 non-null    int64  \n",
      " 12  thal      302 non-null    int64  \n",
      " 13  target    302 non-null    int64  \n",
      "dtypes: float64(1), int64(13)\n",
      "memory usage: 35.4 KB\n",
      "None\n",
      "\n",
      "Cleaned dataset saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\Ashish\\OneDrive\\Desktop\\Jupyter Notebook\\Internship task 6\\heart_disease.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "display(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Handle missing values (e.g., filling with mean for numerical columns)\n",
    "df.fillna(df.mean(numeric_only=True), inplace=True)\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_rows = df.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows: {duplicate_rows}\")\n",
    "\n",
    "# Remove duplicate rows if any\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Display dataset info after cleaning\n",
    "print(\"\\nDataset Info After Cleaning:\")\n",
    "print(df.info())\n",
    "\n",
    "# Save the cleaned dataset (optional)\n",
    "cleaned_file_path = r\"C:\\Users\\Ashish\\OneDrive\\Desktop\\Jupyter Notebook\\Internship task 6\\cleaned_heart_disease.xlsx\"\n",
    "df.to_excel(cleaned_file_path, index=False)\n",
    "print(\"\\nCleaned dataset saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7ed66e-9ef3-4568-9c28-b36db0823d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "b. Feature Engineering\n",
    "○ Normalize or scale numerical features like Age, Cholesterol, and Blood\n",
    "Pressure to improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daaeaa12-feba-407c-a6fa-e9b26ad2f46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scaled Numerical Features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>Blood Pressure</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.708333</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.481132</td>\n",
       "      <td>0.244292</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.283105</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.178082</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.562500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.251142</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.520548</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Age  sex  cp  Blood Pressure  Cholesterol  fbs  restecg  thalach  \\\n",
       "0  0.708333    1   3        0.481132     0.244292    1        0      150   \n",
       "1  0.166667    1   2        0.339623     0.283105    0        1      187   \n",
       "2  0.250000    0   1        0.339623     0.178082    0        0      172   \n",
       "3  0.562500    1   1        0.245283     0.251142    0        1      178   \n",
       "4  0.583333    0   0        0.245283     0.520548    0        1      163   \n",
       "\n",
       "   exang  oldpeak  slope  ca  thal  target  \n",
       "0      0      2.3      0   0     1       1  \n",
       "1      0      3.5      0   0     2       1  \n",
       "2      0      1.4      2   0     2       1  \n",
       "3      0      0.8      2   0     2       1  \n",
       "4      1      0.6      2   0     2       1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed dataset with scaled features saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary library\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (assuming df is already loaded)\n",
    "file_path = r\"C:\\Users\\Ashish\\OneDrive\\Desktop\\Jupyter Notebook\\Internship task 6\\cleaned_heart_disease.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Define numerical features\n",
    "numerical_features = [\"Age\", \"Cholesterol\", \"Blood Pressure\"]\n",
    "\n",
    "# Min-Max Scaling (scales values between 0 and 1)\n",
    "def min_max_scaling(column):\n",
    "    return (column - column.min()) / (column.max() - column.min())\n",
    "\n",
    "# Apply Min-Max Scaling to numerical features\n",
    "for feature in numerical_features:\n",
    "    df[feature] = min_max_scaling(df[feature])\n",
    "\n",
    "# Display scaled features\n",
    "print(\"\\nScaled Numerical Features:\")\n",
    "display(df.head())\n",
    "\n",
    "# Save the processed dataset\n",
    "processed_file_path = r\"C:\\Users\\Ashish\\OneDrive\\Desktop\\Jupyter Notebook\\Internship task 6\\processed_heart_disease.xlsx\"\n",
    "df.to_excel(processed_file_path, index=False)\n",
    "print(\"\\nProcessed dataset with scaled features saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deaa206-4ecc-408f-a1b5-7cc465f80599",
   "metadata": {},
   "outputs": [],
   "source": [
    "c. Model Training\n",
    "○ Train a Logistic Regression model to classify patients as having heart disease\n",
    "or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75d88e05-1c70-4eaf-9fde-53e958c4b04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Columns: Index(['Age', 'sex', 'cp', 'Blood Pressure', 'Cholesterol', 'fbs', 'restecg',\n",
      "       'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Diagnosis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Diagnosis'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m target_column \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDiagnosis\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Change this if your column name is different\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Convert categorical target values (if applicable)\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_column\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m:  \u001b[38;5;66;03m# If the column contains \"Yes\"/\"No\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     df[target_column] \u001b[38;5;241m=\u001b[39m df[target_column]\u001b[38;5;241m.\u001b[39mmap({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYes\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m})\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Define features (X) and target (y)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Diagnosis'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the cleaned dataset\n",
    "file_path = r\"C:\\Users\\Ashish\\OneDrive\\Desktop\\Jupyter Notebook\\Internship task 6\\processed_heart_disease.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Print column names to identify the correct target column\n",
    "print(\"Dataset Columns:\", df.columns)\n",
    "\n",
    "# Replace with the actual column name for heart disease\n",
    "target_column = \"Diagnosis\"  # Change this if your column name is different\n",
    "\n",
    "# Convert categorical target values (if applicable)\n",
    "if df[target_column].dtype == 'object':  # If the column contains \"Yes\"/\"No\"\n",
    "    df[target_column] = df[target_column].map({\"Yes\": 1, \"No\": 0})\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df.drop(columns=[target_column])  # Features\n",
    "y = df[target_column]  # Target (1 = Disease, 0 = No Disease)\n",
    "\n",
    "# Split data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26ce5e0-bcf9-4053-9c44-e29b4460b7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d. Model Evaluation\n",
    "○ Assess model accuracy using a confusion matrix and classification metrics like:\n",
    "■ Precision\n",
    "■ Recall\n",
    "■ F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a39b4b3b-9469-4b38-b847-2cceb5c59d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
      "Using target column: target\n",
      "\n",
      "✅ Model Accuracy: 0.5902\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"C:\\Users\\Ashish\\OneDrive\\Desktop\\Jupyter Notebook\\Internship task 6\\heart_disease.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Clean column names (remove extra spaces)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Print column names to check available columns\n",
    "print(\"Available columns:\", df.columns.tolist())\n",
    "\n",
    "# Choose an existing target column\n",
    "target_column = df.columns[-1]  # Assuming last column is the target (modify if needed)\n",
    "print(f\"Using target column: {target_column}\")\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df.drop(columns=[target_column]).values  # Convert to NumPy array\n",
    "y = df[target_column].values.reshape(-1, 1)  # Convert to column vector\n",
    "\n",
    "# Normalize numerical features (Min-Max scaling)\n",
    "X = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "\n",
    "# Add bias term (column of 1s) for the intercept\n",
    "X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "# Split into training and testing sets (80% train, 20% test)\n",
    "split_index = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "# Initialize weights\n",
    "theta = np.zeros((X_train.shape[1], 1))\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Train using Gradient Descent\n",
    "def train(X, y, theta, lr=0.01, epochs=5000):\n",
    "    m = len(y)\n",
    "    for _ in range(epochs):\n",
    "        h = sigmoid(X @ theta)\n",
    "        gradient = (X.T @ (h - y)) / m\n",
    "        theta -= lr * gradient\n",
    "    return theta\n",
    "\n",
    "# Train model\n",
    "theta = train(X_train, y_train, theta)\n",
    "\n",
    "# Predict\n",
    "def predict(X, theta):\n",
    "    return (sigmoid(X @ theta) >= 0.5).astype(int)\n",
    "\n",
    "y_pred = predict(X_test, theta)\n",
    "\n",
    "# Model evaluation (Accuracy)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"\\n✅ Model Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb82a5b-2eb7-4fd8-8976-e9ca610871cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Deliverables\n",
    "● Logistic Regression Model: Trained model to predict heart disease.\n",
    "● Evaluation Report:\n",
    "○ Confusion matrix\n",
    "○ Accuracy, precision, recall, and F1-score insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7935ba78-dfd4-44d5-91da-fdb9f39a707d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
      "Using target column: target\n",
      "\n",
      "✅ Model Evaluation Results\n",
      "Accuracy: 0.5902\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1-Score: 0.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 0 25]\n",
      " [ 0 36]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"C:\\Users\\Ashish\\OneDrive\\Desktop\\Jupyter Notebook\\Internship task 6\\heart_disease.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Clean column names (remove extra spaces)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Print column names to check available columns\n",
    "print(\"Available columns:\", df.columns.tolist())\n",
    "\n",
    "# Choose an existing target column\n",
    "target_column = df.columns[-1]  # Assuming last column is the target (modify if needed)\n",
    "print(f\"Using target column: {target_column}\")\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df.drop(columns=[target_column]).values  # Convert to NumPy array\n",
    "y = df[target_column].values.reshape(-1, 1)  # Convert to column vector\n",
    "\n",
    "# Normalize numerical features (Min-Max scaling)\n",
    "X = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "\n",
    "# Add bias term (column of 1s) for the intercept\n",
    "X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "# Split into training and testing sets (80% train, 20% test)\n",
    "split_index = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "# Initialize weights\n",
    "theta = np.zeros((X_train.shape[1], 1))\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Train using Gradient Descent\n",
    "def train(X, y, theta, lr=0.01, epochs=5000):\n",
    "    m = len(y)\n",
    "    for _ in range(epochs):\n",
    "        h = sigmoid(X @ theta)\n",
    "        gradient = (X.T @ (h - y)) / m\n",
    "        theta -= lr * gradient\n",
    "    return theta\n",
    "\n",
    "# Train model\n",
    "theta = train(X_train, y_train, theta)\n",
    "\n",
    "# Predict function\n",
    "def predict(X, theta):\n",
    "    return (sigmoid(X @ theta) >= 0.5).astype(int)\n",
    "\n",
    "y_pred = predict(X_test, theta)\n",
    "\n",
    "# Confusion Matrix Calculation\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    return np.array([[TP, FP], [FN, TN]])\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Accuracy, Precision, Recall, F1-Score\n",
    "TP, FP = cm[0, 0], cm[0, 1]\n",
    "FN, TN = cm[1, 0], cm[1, 1]\n",
    "\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "# Print results\n",
    "print(\"\\n✅ Model Evaluation Results\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1_score:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1560bc48-9572-4bc5-982c-856007058990",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
